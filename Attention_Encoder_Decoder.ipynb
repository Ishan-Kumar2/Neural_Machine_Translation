{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attention_Encoder_Decoder.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7Patlq86Mbq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext.datasets import TranslationDataset, Multi30k\n",
        "from torchtext.data import Field, BucketIterator\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import time\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDI1gtDH6_p3",
        "colab_type": "code",
        "outputId": "06916a4c-a2ae-4ba5-d7c0-d518a5c269b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "source": [
        "! python -m spacy download de"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting de_core_news_sm==2.1.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.1.0/de_core_news_sm-2.1.0.tar.gz (11.1MB)\n",
            "\u001b[K     |████████████████████████████████| 11.1MB 717kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: de-core-news-sm\n",
            "  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.1.0-cp36-none-any.whl size=11073065 sha256=a0359fd3e8fa8661f5fd169667147c02014040af13849db7afcb9e900d89ab92\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-0lmft77d/wheels/b4/8b/5e/d2ce5d2756ca95de22f50f68299708009a4aafda2aea79c4e4\n",
            "Successfully built de-core-news-sm\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/de_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/de\n",
            "You can now load the model via spacy.load('de')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jTrkeaw6wlu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spacy_ger=spacy.load('de')\n",
        "spacy_eng=spacy.load('en')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nswYGzJY68xt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_ger(text):\n",
        "\n",
        "  return [tok.text for tok in spacy_ger.tokenizer(text)]\n",
        "\n",
        "def tokenize_eng(text):\n",
        "\n",
        "  return[ tok.text for tok in spacy_eng.tokenizer(text)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJdll_oQ73kp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Creating the Field\n",
        "\n",
        "src=Field(tokenize=tokenize_ger,\n",
        "          init_token='<sos>',\n",
        "          eos_token='<eos>',\n",
        "          lower=True)\n",
        "\n",
        "trg=Field(tokenize=tokenize_eng,\n",
        "          init_token='<sos>',\n",
        "          eos_token='<eos>',\n",
        "          lower=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYo_TCGJ73ng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data,valid_data,test_data=Multi30k.splits(exts=('.de','.en'),fields=(src,trg))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKkVcgCJ73qg",
        "colab_type": "code",
        "outputId": "0d6e8177-5b75-4d86-d2b9-70b3c2f38f50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 58
        }
      },
      "source": [
        "print(vars(train_data.examples[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'src': ['mehrere', 'männer', 'mit', 'schutzhelmen', 'bedienen', 'ein', 'antriebsradsystem', '.'], 'trg': ['several', 'men', 'in', 'hard', 'hats', 'are', 'operating', 'a', 'giant', 'pulley', 'system', '.']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDAORXv473tg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Tokens apprearing less than 2 times\n",
        "src.build_vocab(train_data,min_freq=2)\n",
        "trg.build_vocab(train_data,min_freq=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5Kpg4FWVpsJ",
        "colab_type": "code",
        "outputId": "d41db543-9c28-4fab-ae86-fb46fec50957",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        }
      },
      "source": [
        "src.vocab.stoi['<eos>']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJ-CxNjV73wV",
        "colab_type": "code",
        "outputId": "634047db-8ef4-475d-d90c-8ebdb8f4be1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        }
      },
      "source": [
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msjHGPhy73zL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE=128\n",
        "\n",
        "train_iterator,valid_iterator,test_iterator=BucketIterator.splits((train_data,valid_data,test_data),batch_size=BATCH_SIZE,device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUt18EtSl21A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class StackedGRUEncoder(nn.Module):\n",
        "\n",
        "  def __init__(self,input_dim,emb_dim,enc_hid_size=512,dropout=0.3,n_layers=1,bidirectional=False):\n",
        "    super(StackedGRUEncoder,self).__init__()\n",
        "    self.input_dim=input_dim\n",
        "    self.emb_dim=emb_dim\n",
        "\n",
        "    self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    ##self.src_word_emb=src_emb\n",
        "    self.emb_dim=emb_dim\n",
        "    self.enc_hid_dim=enc_hid_size\n",
        "    \n",
        "    self.bigru=nn.GRU(emb_dim,hidden_size=self.enc_hid_dim,\n",
        "                      num_layers=n_layers,bias=True,\n",
        "                      dropout=dropout,bidirectional=bidirectional)\n",
        "    \n",
        "  def forward(self,src):\n",
        "    \n",
        "    k=self.embedding(src)\n",
        "    \n",
        "    embedded = self.dropout(k)\n",
        "    ##if(src.dim()==3):\n",
        "      ##xs_e=xs\n",
        "    ##else: \n",
        "    xs_e=embedded\n",
        "    ##self.bigru.flatten_parameters()\n",
        "    outputs,hidden=self.bigru(xs_e)\n",
        "    \n",
        "    return hidden\n",
        "    ##*xs_mask[:,:,None]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKZQS00W732T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    \n",
        "    #Bidirectional \n",
        "    def __init__(self, input_dim, emb_dim, enc_hid_dim,dec_hid_dim, dropout=0.3,bidirectional=True):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.enc_hid_dim = enc_hid_dim\n",
        "        self.dec_hid_dim=dec_hid_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, enc_hid_dim,bidirectional=bidirectional)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc=nn.Linear(enc_hid_dim*2,dec_hid_dim)\n",
        "    \n",
        "    def forward(self, src):\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        \n",
        "        outputs, hidden = self.rnn(embedded) #no cell state!\n",
        "        hidden=torch.tanh(self.fc(torch.cat((hidden[-1,:,:],hidden[-2,:,:]),dim=1)))\n",
        "\n",
        "        return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34joFCkboF5c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attention(nn.Module):\n",
        "  def __init__(self,enc_hid_dim,dec_hid_dim):\n",
        "    super().__init__()\n",
        "    self.attn=nn.Linear((enc_hid_dim*2)+dec_hidden_state,dec_hid_dim)\n",
        "    self.v=nn.Parameter(torch.rand(dec_hid_dim))\n",
        "\n",
        "\n",
        "  def forward(self,hidden,encoder_outputs):\n",
        "\n",
        "    batch_size=encoder_outputs.shape[1]\n",
        "    src_len=encoder_outputs.shape[0]\n",
        "\n",
        "    #Hidden here is the prev hidden state of decoder\n",
        "    hidden=hidden.unsqueeze(1)\n",
        "    hidden=hidden.repeat(1,src_len,1)\n",
        "\n",
        "    encoder_outputs=encoder_output.permute(1,0,2)\n",
        "\n",
        "    e=torch.tanh(self.attn(torch.cat((hidden,encoder_outputs),dim=2)))\n",
        "    #energy matrix = [batch size, src len, dec hid dim]\n",
        "    e=e.permute(0,2,1)\n",
        "    #energy matrix = [batch size, dec hid dim, src len]\n",
        "    \n",
        "    v=self.v.repeat(batch_size,1).unsqueeze(1)\n",
        "    #v=[batch_size,1,dec_hid_dim]\n",
        "\n",
        "    attention=torch.bmm(v,energy).squeeze(1)\n",
        "    #bmm is batch matrix multiply\n",
        "\n",
        "    return F.softmax(attention,dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8IGXdm0735P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim + hid_dim, hid_dim)\n",
        "        self.fc_out = nn.Linear(emb_dim + hid_dim * 2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, context):\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        \n",
        "        \n",
        "        \n",
        "        ##embedded=embedded.repeat(4,1,1)\n",
        "        emb_con = torch.cat((embedded, context), dim = 2)\n",
        "            \n",
        "        output, hidden = self.rnn(emb_con, hidden)\n",
        "        \n",
        "        output = torch.cat((embedded.squeeze(0), hidden.squeeze(0), context.squeeze(0)), \n",
        "                           dim = 1)\n",
        "       \n",
        "        prediction = self.fc_out(output)\n",
        "        \n",
        "        return prediction, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOXIR_kV738e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self,encoder,decoder,device):\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder=encoder\n",
        "    self.decoder=decoder\n",
        "    self.device=device\n",
        "\n",
        "\n",
        "  def forward(self, src,trg,teacher_forcing_ratio=0.5):\n",
        "    \n",
        "    batch_size=src.shape[1]\n",
        "    trg_len=trg.shape[0]\n",
        "    trg_vocab_size=self.decoder.output_dim\n",
        "\n",
        "\n",
        "    outputs=torch.zeros(trg_len,batch_size,trg_vocab_size).to(self.device)\n",
        "    context=self.encoder(src)\n",
        "\n",
        "    hidden=context\n",
        "\n",
        "    input=trg[0,:]\n",
        "\n",
        "    for t in range(1,trg_len):\n",
        "\n",
        "      output,hidden=self.decoder(input,hidden,context)\n",
        "      outputs[t]=output\n",
        "\n",
        "      top1=output.argmax(1)\n",
        "\n",
        "      if(random.random()<teacher_forcing_ratio):\n",
        "        input=trg[t]\n",
        "      else:\n",
        "        input=top1\n",
        "\n",
        "    return outputs  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkBGFhm074Ah",
        "colab_type": "code",
        "outputId": "e9438940-709b-4768-fbd2-54c5d095607e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        }
      },
      "source": [
        "Input_dim=len(src.vocab)\n",
        "Output_dim=len(trg.vocab)\n",
        "\n",
        "ENC_EMB_DIM=256\n",
        "DEC_EMB_DIM=256\n",
        "HID_DIM=512\n",
        "ENC_DROPOUT=0.5\n",
        "DEC_DROPOUT=0.5\n",
        "\n",
        "enc=StackedGRUEncoder(Input_dim,ENC_EMB_DIM,HID_DIM,ENC_DROPOUT)\n",
        "\n",
        "dec=Decoder(Output_dim,DEC_EMB_DIM,512,DEC_DROPOUT)\n",
        "\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model=Seq2Seq(enc,dec,device).to(device)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUvEnUQJQ5Fb",
        "colab_type": "code",
        "outputId": "4c9beb18-98d1-4f73-aba2-aec954d7c4ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        }
      },
      "source": [
        "def init_weights(m):\n",
        "  for name, param in m.named_parameters():\n",
        "    nn.init.normal_(param.data,mean=0,std=0.01)\n",
        "\n",
        "model.apply(init_weights)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): StackedGRUEncoder(\n",
              "    (embedding): Embedding(4, 256)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (bigru): GRU(256, 512, dropout=0.5)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(4, 256)\n",
              "    (rnn): GRU(768, 512)\n",
              "    (fc_out): Linear(in_features=1280, out_features=4, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 318
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgpaeHv1Q5Lg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer=optim.Adam(model.parameters())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOFnCZs_Zjxx",
        "colab_type": "code",
        "outputId": "a6c42742-6537-448a-b578-69372e6b35c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "for i, batch in enumerate(train_iterator):\n",
        "  ##src=batch.src\n",
        "  ##trg=batch.trg\n",
        "  print(batch)\n",
        "  break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[torchtext.data.batch.Batch of size 128 from MULTI30K]\n",
            "\t[.src]:[torch.cuda.LongTensor of size 31x128 (GPU 0)]\n",
            "\t[.trg]:[torch.cuda.LongTensor of size 29x128 (GPU 0)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWAQq4_6Q5Rl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRG_PAD_IDX=trg.vocab.stoi[trg.pad_token]\n",
        "criterion=nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "befkQkZZQ5Je",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model,iterator,optimizer,criterion,clip):\n",
        "  model.train()\n",
        "\n",
        "  epoch_loss=0\n",
        "\n",
        "  for i, batch in enumerate(iterator):\n",
        "    src=batch.src\n",
        "    trg=batch.trg\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    output=model(src,trg)\n",
        "\n",
        "    output_dim=output.shape[-1]\n",
        "\n",
        "    output=output[1:].view(-1,output_dim)\n",
        "    trg=trg[1:].view(-1)\n",
        "\n",
        "    loss=criterion(output,trg)\n",
        "    loss.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(),clip)\n",
        "    optimizer.step()\n",
        "\n",
        "    epoch_loss+=loss.item()\n",
        "\n",
        "  return epoch_loss/len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5E361jDSty2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model,iterator,criterion):\n",
        "  \n",
        "  model.eval()\n",
        "\n",
        "  epoch_loss=0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for i, batch in enumerate(iterator):\n",
        "      src=batch.src\n",
        "      trg=batch.trg\n",
        "\n",
        "      \n",
        "      output=model(src,trg,0)\n",
        "\n",
        "      output_dim=output.shape[-1]\n",
        "\n",
        "      output=output[1:].view(-1,output_dim)\n",
        "      trg=trg[1:].view(-1)\n",
        "\n",
        "      loss=criterion(output,trg)\n",
        "      \n",
        "      epoch_loss+=loss.item()\n",
        "\n",
        "  return epoch_loss/len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUGe1Ll9TMYU",
        "colab_type": "code",
        "outputId": "92e4dc1d-cb1d-4c6d-a894-7102aa097abb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        }
      },
      "source": [
        "N_EPOCHS=10\n",
        "CLIP=1\n",
        "\n",
        "best_valid_loss=float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "  train_loss=train(model,train_iterator,optimizer,criterion,CLIP)\n",
        "  valid_loss=evaluate(model,valid_iterator,criterion)\n",
        "\n",
        "  if valid_loss < best_valid_loss:\n",
        "    best_valid_loss = valid_loss\n",
        "\n",
        "  print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "  print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
            "\t Val. Loss: 0.332 |  Val. PPL:   1.394\n",
            "\tTrain Loss: 0.176 | Train PPL:   1.193\n",
            "\t Val. Loss: 0.222 |  Val. PPL:   1.248\n",
            "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
            "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
            "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
            "\t Val. Loss: 0.148 |  Val. PPL:   1.159\n",
            "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
            "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
            "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
            "\t Val. Loss: 0.147 |  Val. PPL:   1.159\n",
            "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
            "\t Val. Loss: 0.151 |  Val. PPL:   1.164\n",
            "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
            "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
            "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
            "\t Val. Loss: 0.147 |  Val. PPL:   1.159\n",
            "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
            "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9H3Alk4hUoD1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}